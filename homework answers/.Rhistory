lambda.min
coef(rr_cv, s = "lambda.min")   # Coefficients at best lambda
coef(rr_cv, s = "lambda.1se")   # Coefficients at simpler lambda
set.seed(123)
rr_cv <- cv.glmnet(x,y, alpha =0)
rr_cv
coef(rr_cv, s = "lambda.min") |>. round(2)   # Coefficients at best lambda
coef(rr_cv, s = "lambda.min") |> round(2)   # Coefficients at best lambda
coef(rr_cv, s = "lambda.1se") |> round(2)   # Coefficients at simpler lambda
coef(rr_cv, s = "lambda.min") |> round(2)
lr <- glmnet(x, y)
lr
help("glmnet")
lr <- glmnet(x, y, alpha = 1)
lr
help("cv.glmnet")
set.seed(123)
lr_cv <- cv.glmnet(x, y, nfolds = 10)
lr_cv
coef(lr_cv)
coef(lr_cv, s = "lambda.min")
set.seed(1234)
Z <- sample(nrow(Boston), .5*nrow(Boston))
boston_train <- Boston[Z,]
boston_test <- Boston[-Z,]
reg <- lm(medv ~ ., data = boston_train)
x <- model.matrix(reg)[,-1]
y <- boston_train$medv
train_r <- glmnet(x, y)
set.seed(123) # for the sampling in cross validation
train_r_cv <- cv.glmnet(x, y)
train_r_cv
plot(train_r_cv)
coef(train_r_cv)
set.seed(123)
n <- nrow(x)
train_index <- sample(1:n, n * 0.5)
test_index <- setdiff(1:n, train_index)
x_train <- x[train_index, ]
y_train <- y[train_index]
x_test  <- x[test_index, ]
y_test  <- y[test_index]
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
best_lambda <- lasso_cv$lambda.min
y_pred <- predict(lasso_cv, s = best_lambda, newx = x_test)
test_mse <- mean((y_test - y_pred)^2)
test_mse
install.packages("pls")
knitr::opts_chunk$set(echo = TRUE)
library(pls)
load("../data/Auto-3.rda")
pcr.fit = pcr(mpg~.-name-origin+as.factor(origin),data=Auto)
lm.fit <- lm(mpg~.-name-origin+as.factor(origin),data=Auto)
pcr.fit
lm.fit
summary(pcr.fit)
auto <- na.omit(Auto)
help("pcr")
lm.fit
summary(lm.fit)
summary(pcr.fit)
summary(lm.fit)
X <-model.matrix(lm.fit)
head(X)
library(stats)
pc <- prcomp(X)
pc
screeplot(pc)
summary(pc)
pc_s <- prcomp(X[,-1], scale = TRUE)
pc_s
pcr_reg <- pcr(mpg~.-name-origin+as.factor(origin),data=Auto, scale=TRUE, validation = "CV")
summary(pcr_reg)
plsr.fit = plsr(mpg~.-name-origin+as.factor(origin),data=Auto)
summary(plsr.fit)
pls_reg <- plsr(mpg~.-name-origin+as.factor(origin),data=Auto, scale = T, validation = "CV")
summary(plsr.fit)
summary(pls_reg)
validationplot(pls_reg)
knitr::opts_chunk$set(echo = TRUE)
# Given values
x <- 4
mu_yes <- 10
mu_no <- 0
sigma <- 6
p_yes <- 0.8
p_no <- 0.2
f_yes <- (1 / (sqrt(2 * pi) * sigma)) * exp(-((x - mu_yes)^2) / (2 * sigma^2))
f_no <- (1 / (sqrt(2 * pi) * sigma)) * exp(-((x - mu_no)^2) / (2 * sigma^2))
posterior_yes <- (f_yes * p_yes) / (f_yes * p_yes + f_no * p_no)
# Print all results
f_yes
f_no
posterior_yes
library(ISLR)
attach(weekly)
attach(Weekly)
Weekly <- attach(Weekly)
View(Weekly)
View(Weekly)
Weekly[["Direction"]]
test <- tibble(Weekly)
View(Weekly)
library(tibble)
test <- tibble(Weekly)
test <- as_tibble(Weekly)
class(Weekly)
View(Weekly)
str(Weekly)
View(Weekly)
Weekly[["Direction"]]
df$directions <- Weekly[["Direction"]]
df <- Weekly[["Direction"]]
data <- tibble(df)
View(data)
library(ISLR)
library(tibble)
Direction <- Weekly[["Direction"]]
df <- tibble(Direction)
df$Lag1 <- Weekly[["Lag1"]]
View(df)
library(ISLR)
library(tibble)
Direction <- Weekly[["Direction"]]
df <- tibble(Direction)
df$Lag1 <- Weekly[["Lag1"]]
df$Lag2 <- Weekly[["Lag2"]]
df$Lag3 <- Weekly[["Lag3"]]
df$Lag4 <- Weekly[["Lag4"]]
df$Lag5 <- Weekly[["Lag5"]]
df$Year <- Weekly[["Year"]]
df$Volume <- Weekly[["Volume"]]
df$Today <- Weekly[["Today"]]
View(df)
logit_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(logit_model)
logit_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = Weekly, family = binomial)
summary(logit_model)
pred_probs <- predict(logit_model, type = "response")
pred_probs
glimpse(Weekly)
logit_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = df, family = binomial)
summary(logit_model)
glimpse(df)
logit_model <- glm(Direction ~ Lag1 + Lag2 + Lag3 + Lag4 + Lag5 + Volume, data = df, family = binomial)
summary(logit_model)
table(df$Direction)
View(data)
View(df)
pred_probs <- predict(logit_model, type = "response")
pred_class <- ifelse(pred_probs > 0.5, "Up", "Down")
conf_matrix <- table(Predicted = pred_class, Actual = Weekly$Direction)
conf_matrix
accuracy <- mean(pred_class == Weekly$Direction)
accuracy
summary(Weekly$Year)
library(tidyverse)
summary(df$Year)
pred_probs <- predict(logit_model, type = "response")
pred_class <- ifelse(pred_probs > 0.5, "Up", "Down")
conf_matrix <- table(Predicted = pred_class, Actual = df$Direction)
conf_matrix
accuracy <- mean(pred_class == df$Direction)
accuracy
# Create training and test sets
train <- df$Year <= 2008
test  <- df$Year > 2008
# Check sizes
table(train, df$Year)  # optional
logit_train <- glm(Direction ~ Lag2, data = df, subset = train, family = binomial)
# Create training and test sets
train <- df$Year <= 2008
test  <- df$Year > 2008
logit_train <- glm(Direction ~ Lag2, data = df, subset = train, family = binomial)
test_probs <- predict(logit_train, newdata = df[test, ], type = "response")
test_pred <- ifelse(test_probs > 0.5, "Up", "Down")
# Create training and test sets
train <- df$Year <= 2008
test  <- df$Year > 2008
logit_train <- glm(Direction ~ Lag2, data = df, subset = train, family = binomial)
test_probs <- predict(logit_train, newdata = df[test, ], type = "response")
test_pred <- ifelse(test_probs > 0.5, "Up", "Down")
# Actual values for test set
actual_test <- df$Direction[test]
# Confusion matrix
conf_matrix <- table(Predicted = test_pred, Actual = actual_test)
conf_matrix
# Overall accuracy
accuracy <- mean(test_pred == actual_test)
accuracy
library(pROC)
library(pROC)
actual_binary <- ifelse(actual_test == "Up", 1, 0)
roc_obj <- roc(actual_binary, test_probs)
plot(roc_obj, col = "blue", main = "ROC Curve - Logistic Regression")
library(pROC)
actual_binary <- ifelse(actual_test == "Up", 1, 0)
roc_obj <- roc(actual_binary, test_probs)
plot(roc_obj)
library(class)
library(caret)
set.seed(1)
ks <- 1:10
acc <- numeric(length(ks))
for (i in ks) {
knn_pred <- knn(train = train_X, test = test_X, cl = train_Y, k = i)
acc[i] <- mean(knn_pred == test_Y)
}
library(class)
library(caret)
lag_vars <- c("Lag1", "Lag2", "Lag3", "Lag4", "Lag5")
train_index <- df$Year <= 2008
test_index  <- df$Year > 2008
train_X <- scale(Weekly[train_index, lag_vars])
library(class)
library(caret)
lag_vars <- c("Lag1", "Lag2", "Lag3", "Lag4", "Lag5")
train_index <- df$Year <= 2008
test_index  <- df$Year > 2008
train_X <- scale(df[train_index, lag_vars])
test_X  <- scale(df[test_index, lag_vars])
train_Y <- df$Direction[train_index]
test_Y  <- df$Direction[test_index]
set.seed(1)
ks <- 1:10
acc <- numeric(length(ks))
for (i in ks) {
knn_pred <- knn(train = train_X, test = test_X, cl = train_Y, k = i)
acc[i] <- mean(knn_pred == test_Y)
}
data.frame(k = ks, Accuracy = round(acc, 4))
best_k <- ks[which.max(acc)]
best_k
knn_final <- knn(train = train_X, test = test_X, cl = train_Y, k = best_k)
table(Predicted = knn_final, Actual = test_Y)
mean(knn_final == test_Y)
library(MASS)
lda_model <- lda(Direction ~ Lag2, data = Weekly, subset = train_index)
library(MASS)
lda_model <- lda(Direction ~ Lag2, data = df, subset = train_index)
lda_pred <- predict(lda_model, Weekly[test_index, ])
library(MASS)
lda_model <- lda(Direction ~ Lag2, data = df, subset = train_index)
lda_pred <- predict(lda_model, df[test_index, ])
conf_matrix <- table(Predicted = lda_pred$class, Actual = Weekly$Direction[test_index])
print(conf_matrix)
accuracy <- mean(lda_pred$class == Weekly$Direction[test_index])
print(accuracy)
library(MASS)
lda_model <- lda(Direction ~ Lag2, data = df, subset = train_index)
lda_pred <- predict(lda_model, df[test_index, ])
conf_matrix <- table(Predicted = lda_pred$class, Actual = df$Direction[test_index])
print(conf_matrix)
accuracy <- mean(lda_pred$class == df$Direction[test_index])
print(accuracy)
qda_model <- lda(Direction ~ Lag2, data = df, subset = train_index)
qda_pred <- predict(lda_model, df[test_index, ])
conf_matrix <- table(Predicted = qda_pred$class, Actual = df$Direction[test_index])
print(conf_matrix)
accuracy <- mean(qda_pred$class == df$Direction[test_index])
print(accuracy)
qda_model <- qda(Direction ~ Lag2, data = df, subset = train_index)
qda_pred <- predict(lda_model, df[test_index, ])
conf_matrix <- table(Predicted = qda_pred$class, Actual = df$Direction[test_index])
print(conf_matrix)
accuracy <- mean(qda_pred$class == df$Direction[test_index])
print(accuracy)
qda_model <- qda(Direction ~ Lag2, data = df, subset = train_index)
qda_pred <- predict(qda_model, df[test_index, ])
conf_matrix <- table(Predicted = qda_pred$class, Actual = df$Direction[test_index])
print(conf_matrix)
accuracy <- mean(qda_pred$class == df$Direction[test_index])
print(accuracy)
set.seed(1)
knn_pred <- knn(train = train_X, test = test_X, cl = train_Y, k = 1)
conf_matrix_knn1 <- table(Predicted = knn_pred, Actual = test_Y)
print(conf_matrix_knn1)
accuracy_knn1 <- mean(knn_pred == test_Y)
print(accuracy_knn1)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
data("Boston")
cor(Boston)
lm.ridge(medv ~., data = Boston)$coef |> round(2)
lm.ridge(medv ~., lambda = 3, data = Boston)$coef |> round(2)
lm.ridge(medv ~., lambda = 10, data = Boston)$coef |> round(2)
lm.ridge(medv ~., lambda = 100, data = Boston)$coef |> round(2)
library(glmnet)
x <- model.matrix(reg)
library(glmnet)
x <- model.matrix(reg)
library(glmnet)
reg <- lm(medv ~ ., data = Boston)
x <- model.matrix(reg)
dim(x)
rr <- lm.ridge(medv ~., lambda = seq(0, 100, 1), data = Boston)
select(rr)
knitr::opts_chunk$set(echo = TRUE)
library(MASS)
library(tidyverse)
data("Boston")
cor(Boston)
lm.ridge(medv ~., data = Boston)$coef |> round(2)
lm.ridge(medv ~., lambda = 3, data = Boston)$coef |> round(2)
lm.ridge(medv ~., lambda = 10, data = Boston)$coef |> round(2)
lm.ridge(medv ~., lambda = 100, data = Boston)$coef |> round(2)
rr <- lm.ridge(medv ~., lambda = seq(0, 100, 1), data = Boston)
select(rr)
rr <-  lm.ridge(medv ~ ., lambda = seq(0, 100, 1), data = Boston)
select(rr)
rr <- lm.ridge(medv ~., lambda = seq(0, 100, 1), data = Boston)
select(rr)
library(MASS)
library(MASS)
rr <- lm.ridge(medv ~., lambda = seq(0, 100, 1), data = Boston)
select(rr)
rr <- lm.ridge(medv ~., lambda = seq(0, 100, 1), data = Boston)
MASS::select(rr)
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
attach(OJ)
OJ <- OJ
OJ
library(tidyverse)
glimpse(OJ)
help("sample")
knitr::opts_chunk$set(echo = TRUE)
train_index <- sample(1:nrow(Auto), nrow(OJ)/2)
set.seed(123)
train_index <- sample(1:nrow(Auto), nrow(OJ)/2)
train_index <- sample(1:nrow(OJ), nrow(OJ)/2)
train_data <- OJ[train_index, ]
test_data <- OJ[-train_index, ]
train_index <- sample(1:nrow(OJ), 800)
train_data <- OJ[train_index, ]
test_data <- OJ[-train_index, ]
set.seed(123)
train_index <- sample(1:nrow(OJ), 800)
train_data <- OJ[train_index, ]
test_data <- OJ[-train_index, ]
library(tree)
glimspe(OJ)
glimpse(OJ)
tree_model <- tree(Purchase ~ . ,data = OJ) # except for buy as predictors but dont know which one that is.
summary(tree_model)
plot(tree_model)
plot(tree_model)
text(tree_model)
table(Purchase)
glimpse(OJ)
tree_model
tree_model
summary(Yhat)
Yhat <- predict(tree_model, newdata = test_data)
summary(Yhat)
?OJ
Yhat <- predict(tree_model, newdata = test_data, type = "class")
summary(Yhat)
table(Yhat, test_data$Purchase)
mean(Yhat != train_data$Purchase)
length(Yhat)
length(train_data)
nrow(train_data)
length(test_data)
nrow(test_data)
Yhat_tibble <- tibble(Yhat)
View(Yhat_tibble)
table(Yhat_tibble, test_data$Purchase)
table(Yhat, test_data$Purchase)
mean(Yhat_tibble != train_data$Purchase)
mean(Yhat != train_data$Purchase)
(30+15)+(151+30+15+74)
(30+15)/(151+30+15+74)
cv <- cv.tree(tree_model)
cv
cv$size[which.min(cv$dev)]
help("cv.tree")
cv <- cv.tree(tree_model, FUN = prune.misclass)
cv
cv <- cv.tree(tree_model)
cv
cv <- cv.tree(tree_model, FUN = prune.misclass)
cv
plot(cv$size, cv$dev)
plot(cv$size, cv$dev, type = "b")
min_index <- which.min(cv$dev)
best_size <- cv$size[min_index]
min_error <- cv$dev[min_index]
min_index
best_size
min_error
cv
min_error <- cv$dev[min_index] / nrow(OJ)
min_index <- which.min(cv$dev)
best_size <- cv$size[min_index]
min_error <- cv$dev[min_index] / nrow(OJ)
best_size
min_error
tr_opt <- prune.tree(tree_model, best =8)
summary(tr_opt)
cv <- cv.tree(tree_model)
cv
cv <- cv.tree(tree_model, FUN = prune.misclass)
cv
X <- c(0.1, 0.15, 0.2, 0.2, 0.55, 0.6, 0.6,
0.65, 0.7, 0.75)
mean(X)
knitr::include_graphics("Screenshot 2025-06-17 at 10.31.35PM.png")
knitr::include_graphics("Screenshot 2025-06-17 at 10.31.35 PM.png")
knitr::include_graphics("Screenshot 2025-06-17 at 10.31.35â€¯PM.png")
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(tidyverse)
load("../data/Auto-3.rda")
attach(Auto)
glimpse(Auto)
Auto <- Auto%>%
select(-name)
rf <- randomForest(mpg ~ ., data = Auto)
rf
importance(rf)
varImpPlot(rf)
set.seed(123)
train_index <- sample(1:nrow(OJ), nrow(OJ)/2)
install.packages("e1071")
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
load("../data/Auto-3.rda")
ECO = ifelse( mpg > 22.75, "Economy", "Consuming")
attach(Auto)
ECO = ifelse( mpg > 22.75, "Economy", "Consuming")
plot(weight, horsepower)
help("plot")
plot(weight, horsepower, lwd = 3)
plot(weight, horsepower, lwd = 10)
plot(weight, horsepower)
Auto$ECO < as.factor(ECO)
Auto$ECO <- as.factor(ECO)
plot(weight, horsepower, col =ECO )
plot(weight, horsepower, col = as.numeric(ECO))
glimpse(Auto)
glimpse(Auto)
library(tidyverse)
glimpse(Auto)
table(ECO)
plot(weight, horsepower
plot(weight, horsepower
plot(weight, horsepower)
plot(weight, horsepower, col(ECO))
plot(weight, horsepower, col(Auto$ECO))
plot(weight, horsepower, col = ECO)
plot(weight, horsepower, col = as.numeric(ECO))
plot(weight, horsepower, col = as.numeric(Auto$ECO))
ECO = ifelse( mpg > 22.75, "Economy", "Consuming")
Auto$ECO <- as.factor(ECO)
library(e1071)
library(tidyverse)
load("../data/Auto-3.rda")
attach(Auto)
ECO = ifelse( mpg > 22.75, "Economy", "Consuming")
Auto$ECO <- as.factor(ECO)
plot(weight, horsepower, col = as.numeric(Auto$ECO))
rm(ECO)
svm <- svm(ECO ~., data = Auto)
svm
plot(weight, horsepower, col = as.numeric(Auto$ECO))
d = data.frame(ECO, weight, horsepower)
d = data.frame(Auto$ECO, weight, horsepower)
svm <- svm(ECO ~., data = Auto)
summary(svm)
plot(svm)
svm <- svm(ECO ~., data = d)
svm <- svm(d$ECO ~., data = d)
svm <- svm(ECO ~., data = d)
View(d)
d = data.frame(ECO, weight, horsepower)
knitr::opts_chunk$set(echo = TRUE)
library(e1071)
library(tidyverse)
load("../data/Auto-3.rda")
attach(Auto)
ECO = ifelse( mpg > 22.75, "Economy", "Consuming")
Auto$ECO <- as.factor(ECO)
rm(ECO)
attach(Auto)
svm <- svm(ECO ~., data = Auto)
svm
plot(weight, horsepower, col = as.numeric(Auto$ECO))
d = data.frame(ECO, weight, horsepower)
svm <- svm(ECO ~., data = d)
summary(svm)
plot(svm)
View(d)
plot(svm)
plot(svm)
plot(svm, data = Auto[, c(2, 7, 12)])
View(Auto)
plot(svm, data = Auto[, c(4, 5, 10)])
d = data.frame(ECO, weight, horsepower)
svm <- svm(ECO ~., data = d, kernel = "linear")
summary(svm)
plot(svm, data = Auto[, c(4, 5, 10)])
help("svm")
svm_poly <- svm(ECO ~., data = d, kernel = "polynomial")
plot(svm_poly, data = Auto[, c(4, 5, 10)])
svm_rb <- svm(ECO ~., data = d, kernel = "radial basis")
svm_rb <- svm(ECO ~., data = d, kernel = "radial basis")
svm_rb <- svm(ECO ~., data = d, kernel = "sigmoid")
plot(svm_rb, data = Auto[, c(4, 5, 10)])
svm_s <- svm(ECO ~., data = d, kernel = "sigmoid")
plot(svm_s, data = Auto[, c(4, 5, 10)])
